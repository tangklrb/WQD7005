{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'https://www.imdb.com'\n",
    "root_url = host + '/search/title/?languages=en&title_type=feature&genres={}&start={}'\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X '\n",
    "                         '10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                         'Chrome/72.0.3626.109 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>other</th>\n",
       "      <th>comment</th>\n",
       "      <th>movie_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/10</td>\n",
       "      <td>The franchise has had a lobotomy</td>\n",
       "      <td>/review/rw4751363/?ref_=tt_urv</td>\n",
       "      <td>rw4751363</td>\n",
       "      <td>ur101048846</td>\n",
       "      <td>30 March 2019</td>\n",
       "      <td>Warning: Spoilers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dinosaurs. Amusement Park. Tourists. Disaster....</td>\n",
       "      <td>tt0369610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/10</td>\n",
       "      <td>There is a plus...</td>\n",
       "      <td>/review/rw3844587/?ref_=tt_urv</td>\n",
       "      <td>rw3844587</td>\n",
       "      <td>ur22419131</td>\n",
       "      <td>29 October 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>... and it's dinosaurs. Absolutely everything ...</td>\n",
       "      <td>tt0369610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/10</td>\n",
       "      <td>Spielberg Magic, This Is Not. Still, a Visit t...</td>\n",
       "      <td>/review/rw4200129/?ref_=tt_urv</td>\n",
       "      <td>rw4200129</td>\n",
       "      <td>ur35359466</td>\n",
       "      <td>12 June 2018</td>\n",
       "      <td>Warning: Spoilers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You may have heard some critics champion Juras...</td>\n",
       "      <td>tt0369610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3/10</td>\n",
       "      <td>Another piece of modern trash.</td>\n",
       "      <td>/review/rw3846832/?ref_=tt_urv</td>\n",
       "      <td>rw3846832</td>\n",
       "      <td>ur9357474</td>\n",
       "      <td>1 November 2017</td>\n",
       "      <td>Warning: Spoilers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There is a young, handsome (I suspect), super ...</td>\n",
       "      <td>tt0369610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3/10</td>\n",
       "      <td>Bleah</td>\n",
       "      <td>/review/rw3387151/?ref_=tt_urv</td>\n",
       "      <td>rw3387151</td>\n",
       "      <td>ur33389853</td>\n",
       "      <td>3 January 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A dull monster movie without ideas, with all t...</td>\n",
       "      <td>tt0369610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482077</th>\n",
       "      <td>1/10</td>\n",
       "      <td>Low budget junk</td>\n",
       "      <td>/review/rw5063745/?ref_=tt_urv</td>\n",
       "      <td>rw5063745</td>\n",
       "      <td>ur38716160</td>\n",
       "      <td>16 August 2019</td>\n",
       "      <td>Warning: Spoilers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That's the best words I can find to describe i...</td>\n",
       "      <td>tt9904820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482078</th>\n",
       "      <td>2/10</td>\n",
       "      <td>its a surviving story if in the right hands an...</td>\n",
       "      <td>/review/rw4823075/?ref_=tt_urv</td>\n",
       "      <td>rw4823075</td>\n",
       "      <td>ur79950921</td>\n",
       "      <td>2 May 2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is a c-level horror flick, and like most ...</td>\n",
       "      <td>tt9904820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482079</th>\n",
       "      <td>NaN</td>\n",
       "      <td>It don't add up</td>\n",
       "      <td>/review/rw4842487/?ref_=tt_urv</td>\n",
       "      <td>rw4842487</td>\n",
       "      <td>ur26490810</td>\n",
       "      <td>11 May 2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Look, I ain't seen this movie, neither I will,...</td>\n",
       "      <td>tt9904820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482080</th>\n",
       "      <td>10/10</td>\n",
       "      <td>Loved this movie!</td>\n",
       "      <td>/review/rw5817238/?ref_=tt_urv</td>\n",
       "      <td>rw5817238</td>\n",
       "      <td>ur119650852</td>\n",
       "      <td>12 June 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Very intense social drama. Realistic character...</td>\n",
       "      <td>tt9913660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482081</th>\n",
       "      <td>8/10</td>\n",
       "      <td>Beautifully filmed story of Edgar Snow in China</td>\n",
       "      <td>/review/rw5381302/?ref_=tt_urv</td>\n",
       "      <td>rw5381302</td>\n",
       "      <td>ur22484170</td>\n",
       "      <td>4 January 2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Red Star Over China is such a famous book that...</td>\n",
       "      <td>tt9916428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333656 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                              title  \\\n",
       "0        4/10                   The franchise has had a lobotomy   \n",
       "1        2/10                                 There is a plus...   \n",
       "2        7/10  Spielberg Magic, This Is Not. Still, a Visit t...   \n",
       "3        3/10                     Another piece of modern trash.   \n",
       "4        3/10                                              Bleah   \n",
       "...       ...                                                ...   \n",
       "482077   1/10                                    Low budget junk   \n",
       "482078   2/10  its a surviving story if in the right hands an...   \n",
       "482079    NaN                                    It don't add up   \n",
       "482080  10/10                                  Loved this movie!   \n",
       "482081   8/10    Beautifully filmed story of Edgar Snow in China   \n",
       "\n",
       "                                   url  review_id      user_id  \\\n",
       "0       /review/rw4751363/?ref_=tt_urv  rw4751363  ur101048846   \n",
       "1       /review/rw3844587/?ref_=tt_urv  rw3844587   ur22419131   \n",
       "2       /review/rw4200129/?ref_=tt_urv  rw4200129   ur35359466   \n",
       "3       /review/rw3846832/?ref_=tt_urv  rw3846832    ur9357474   \n",
       "4       /review/rw3387151/?ref_=tt_urv  rw3387151   ur33389853   \n",
       "...                                ...        ...          ...   \n",
       "482077  /review/rw5063745/?ref_=tt_urv  rw5063745   ur38716160   \n",
       "482078  /review/rw4823075/?ref_=tt_urv  rw4823075   ur79950921   \n",
       "482079  /review/rw4842487/?ref_=tt_urv  rw4842487   ur26490810   \n",
       "482080  /review/rw5817238/?ref_=tt_urv  rw5817238  ur119650852   \n",
       "482081  /review/rw5381302/?ref_=tt_urv  rw5381302   ur22484170   \n",
       "\n",
       "                   date            spoiler  other  \\\n",
       "0         30 March 2019  Warning: Spoilers    NaN   \n",
       "1       29 October 2017                NaN    NaN   \n",
       "2          12 June 2018  Warning: Spoilers    NaN   \n",
       "3       1 November 2017  Warning: Spoilers    NaN   \n",
       "4        3 January 2016                NaN    NaN   \n",
       "...                 ...                ...    ...   \n",
       "482077   16 August 2019  Warning: Spoilers    NaN   \n",
       "482078       2 May 2019                NaN    NaN   \n",
       "482079      11 May 2019                NaN    NaN   \n",
       "482080     12 June 2020                NaN    NaN   \n",
       "482081   4 January 2020                NaN    NaN   \n",
       "\n",
       "                                                  comment   movie_id  \n",
       "0       Dinosaurs. Amusement Park. Tourists. Disaster....  tt0369610  \n",
       "1       ... and it's dinosaurs. Absolutely everything ...  tt0369610  \n",
       "2       You may have heard some critics champion Juras...  tt0369610  \n",
       "3       There is a young, handsome (I suspect), super ...  tt0369610  \n",
       "4       A dull monster movie without ideas, with all t...  tt0369610  \n",
       "...                                                   ...        ...  \n",
       "482077  That's the best words I can find to describe i...  tt9904820  \n",
       "482078  This is a c-level horror flick, and like most ...  tt9904820  \n",
       "482079  Look, I ain't seen this movie, neither I will,...  tt9904820  \n",
       "482080  Very intense social drama. Realistic character...  tt9913660  \n",
       "482081  Red Star Over China is such a famous book that...  tt9916428  \n",
       "\n",
       "[333656 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = pd.read_csv('data/movies.csv', index_col = 0)\n",
    "reviews_df = pd.read_csv('data/reviews.csv', index_col = 0)\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>name</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>metascore</th>\n",
       "      <th>critic_count</th>\n",
       "      <th>review_count</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_url</th>\n",
       "      <th>review_crawled_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>tt10887282</td>\n",
       "      <td>Kasanova</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/title/tt10887282/reviews</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>tt8972556</td>\n",
       "      <td>I Am Vengeance: Retaliation</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9 critic</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>/title/tt8972556/reviews</td>\n",
       "      <td>85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>tt6853934</td>\n",
       "      <td>Hammer</td>\n",
       "      <td>5.6</td>\n",
       "      <td>73.0</td>\n",
       "      <td>10 critic</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>/title/tt6853934/reviews</td>\n",
       "      <td>85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>tt8636456</td>\n",
       "      <td>Bannister DollHouse</td>\n",
       "      <td>5.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7 critic</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>/title/tt8636456/reviews</td>\n",
       "      <td>91.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>tt7095476</td>\n",
       "      <td>Waiting for Anya</td>\n",
       "      <td>5.5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>11 critic</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>/title/tt7095476/reviews</td>\n",
       "      <td>92.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>tt8484012</td>\n",
       "      <td>Sword of Trust</td>\n",
       "      <td>6.3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>44 critic</td>\n",
       "      <td>36</td>\n",
       "      <td>35</td>\n",
       "      <td>/title/tt8484012/reviews</td>\n",
       "      <td>97.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>tt3246874</td>\n",
       "      <td>Wish Man</td>\n",
       "      <td>7.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6 critic</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>/title/tt3246874/reviews</td>\n",
       "      <td>98.913043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        movie_id                         name  average_rating  metascore  \\\n",
       "1897  tt10887282                     Kasanova             NaN        NaN   \n",
       "70     tt8972556  I Am Vengeance: Retaliation             3.7        NaN   \n",
       "950    tt6853934                       Hammer             5.6       73.0   \n",
       "1232   tt8636456          Bannister DollHouse             5.1        NaN   \n",
       "1132   tt7095476             Waiting for Anya             5.5       38.0   \n",
       "695    tt8484012               Sword of Trust             6.3       70.0   \n",
       "535    tt3246874                     Wish Man             7.1        NaN   \n",
       "\n",
       "     critic_count review_count  review_id                 review_url  \\\n",
       "1897          NaN            0          1  /title/tt10887282/reviews   \n",
       "70       9 critic            7          6   /title/tt8972556/reviews   \n",
       "950     10 critic            7          6   /title/tt6853934/reviews   \n",
       "1232     7 critic           12         11   /title/tt8636456/reviews   \n",
       "1132    11 critic           13         12   /title/tt7095476/reviews   \n",
       "695     44 critic           36         35   /title/tt8484012/reviews   \n",
       "535      6 critic           92         91   /title/tt3246874/reviews   \n",
       "\n",
       "      review_crawled_rate  \n",
       "1897             0.000000  \n",
       "70              85.714286  \n",
       "950             85.714286  \n",
       "1232            91.666667  \n",
       "1132            92.307692  \n",
       "695             97.222222  \n",
       "535             98.913043  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_review_crawled_rate(review_id, review_count):\n",
    "    if review_count == '' or int(review_count) == 0: \n",
    "        return 0\n",
    "    else:\n",
    "        return int(review_id) / int(review_count) * 100\n",
    "\n",
    "movies_df.drop_duplicates(['movie_id'], inplace = True)\n",
    "\n",
    "reviews_df.drop_duplicates(['review_id'], inplace = True)\n",
    "reviews_crawled = reviews_df.groupby('movie_id').count()\n",
    "reviews_count = pd.merge(movies_df, reviews_crawled, on = 'movie_id')[[\n",
    "    'movie_id', 'name', 'average_rating', 'metascore', 'critic_count', 'review_count', 'review_id', 'review_url'\n",
    "]]\n",
    "reviews_count['review_count'] = reviews_count['review_count'].str.replace(',', '').str.extract('(\\d+)').fillna(\"0\")\n",
    "reviews_count['review_crawled_rate'] = reviews_count.apply(\n",
    "    lambda x: calculate_review_crawled_rate(x['review_id'], x['review_count']),\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "movies_with_incomplete_reviews = reviews_count[(reviews_count['review_crawled_rate'] < 99)].sort_values('review_crawled_rate')\n",
    "movies_with_incomplete_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment are loaded via ajax for page 2++, need web driver to crawl\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# function to extract review details from beautiful soup object (shared by normal crawling & web driver crawling)\n",
    "def extract_reviews(movie_id, bs):\n",
    "    movie_reviews = reviews_df[reviews_df['movie_id'] == movie_id]\n",
    "    \n",
    "    reviews = list()\n",
    "    # the beautiful soup object is a list of review\n",
    "    for review_div in bs.find_all(\"div\", class_=\"review-container\"):\n",
    "        # define empty row for review\n",
    "        review = {\n",
    "            \"rating\": \"\", \"title\": \"\", \"url\": \"\", \"id\": \"\", \"user_id\": \"\", \"user_name\": \"\", \n",
    "            \"user_url\": \"\", \"date\": \"\", \"spoiler\": \"\", \"other\": \"\", \"comment\": \"\"\n",
    "        }\n",
    "            \n",
    "        # extracted information from html tags\n",
    "        review_rating = review_div.find_all(\"span\", class_=\"rating-other-user-rating\")\n",
    "        review_link = review_div.find_all(\"a\", class_=\"title\")\n",
    "        review_user = review_div.find_all(\"span\", class_=\"display-name-link\")\n",
    "        review_date = review_div.find_all(\"span\", class_=\"review-date\")\n",
    "        review_spoiler = review_div.find_all(\"span\", class_=\"spoiler-warning\")\n",
    "        review_comment = review_div.select(\"div.content > div.text\")\n",
    "\n",
    "        # and if respected information exists, assign to the row\n",
    "        if len(review_rating) > 0:\n",
    "            review['rating'] = review_rating[0].text.strip()\n",
    "\n",
    "        if len(review_link) > 0:\n",
    "            review['title'] = review_link[0].text.strip()\n",
    "            review['url'] = review_link[0].get('href')\n",
    "            review['id'] = review_link[0].get('href').split('/')[2]\n",
    "            \n",
    "            # skip if review has already been crawled\n",
    "            if review['id'] in movie_reviews['review_id']:\n",
    "                continue\n",
    "\n",
    "        if len(review_user) > 0:\n",
    "            review_user_link = review_user[0].select(\"a\")\n",
    "            if len(review_user_link) > 0:\n",
    "                review['user_name'] = review_user_link[0].text.strip()\n",
    "                review['user_url'] = review_user_link[0].get('href')\n",
    "                review['user_id'] = review_user_link[0].get('href').split('/')[2]\n",
    "            \n",
    "        if len(review_date) > 0:\n",
    "            review['date'] = review_date[0].text.strip()\n",
    "            \n",
    "        if len(review_spoiler) > 0:\n",
    "            review['spoiler'] = review_spoiler[0].text.strip()\n",
    "\n",
    "        if len(review_comment) > 0:\n",
    "            review['comment'] = review_comment[0].text.strip()\n",
    "            \n",
    "        reviews.append(review)\n",
    "    \n",
    "    # return the list of review extraceted from the beautiful soup object\n",
    "    return(reviews)\n",
    "        \n",
    "    \n",
    "# function to crawl review of the given movie & url\n",
    "def crawl_reviews(movie_id, review_url):\n",
    "    \n",
    "    # some issue with crawling process, needs to handle retries\n",
    "    retry_strategy = Retry(\n",
    "        total = 10,\n",
    "        status_forcelist = [429, 500, 502, 503, 504],\n",
    "        method_whitelist = [\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "        backoff_factor = 1\n",
    "    )\n",
    "    \n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "\n",
    "    review_url = host + review_url\n",
    "    print(review_url, end = \" - \")\n",
    "    \n",
    "    # get the beautifu soup of jects of the review page\n",
    "    reviews_bs4 = BeautifulSoup(http.get(review_url, headers=headers).text, 'html.parser')\n",
    "    \n",
    "    # extract review information from the beautiful soup object\n",
    "    reviews = extract_reviews(movie_id, reviews_bs4)\n",
    "    \n",
    "    # dump the first page of review into individual json file for movie\n",
    "    with open('data/reviews_' + movie_id + '.json', 'a+') as reviews_json:\n",
    "        print(json.dumps(reviews), file = reviews_json, flush = True)\n",
    "        \n",
    "    page_count = 0\n",
    "    \n",
    "    # if the page contains reviews\n",
    "    if len(reviews) > 0:\n",
    "        page_count = 1\n",
    "        \n",
    "        # initial the web drive to simulate click to retrieve ajax update\n",
    "        driver = webdriver.Chrome('/home/kitlim/.wdm/drivers/chromedriver/linux64/80.0.3987.106/chromedriver')\n",
    "        wait = WebDriverWait(driver,10)\n",
    "        driver.get(review_url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        # infinite loop until no more new review pages\n",
    "        previous_key = \"\"\n",
    "        while True:\n",
    "            try:\n",
    "                # find if \"Load More\" button exist\n",
    "                element = driver.find_element_by_class_name(\"load-more-data\");\n",
    "                key = element.get_attribute('data-key')\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                # if not, end the loop\n",
    "                break\n",
    "                \n",
    "            # if the key of the current \"Load More\" button is not the same with previous key\n",
    "            if key != previous_key:\n",
    "                if key is not None:\n",
    "                    # get the ajax url for the new review page\n",
    "                    load_more_review_ajax_url = review_url + \"/_ajax?ref_=undefined&paginationKey=\" + key\n",
    "                \n",
    "                    try:\n",
    "                        # get the beautifu soup of jects of the review page returned by the ajax call\n",
    "                        reviews_bs4 = BeautifulSoup(http.get(load_more_review_ajax_url, headers=headers).text, 'html.parser')\n",
    "                    except: \n",
    "                        # error handling\n",
    "                        print('ConnectionError: retry')\n",
    "                        time.sleep(0.5)\n",
    "                        continue\n",
    "                        \n",
    "                    # extract review information from the beautiful soup object\n",
    "                    reviews = extract_reviews(movie_id, reviews_bs4)\n",
    "                    \n",
    "                    page_count = page_count + 1\n",
    "                \n",
    "                # if same with previous key, do nothing (previous ajax call is not completed successfully yet)\n",
    "                else: \n",
    "                    break\n",
    "                \n",
    "                # dump the current page of review into individual json file for movie\n",
    "                with open('data/reviews_' + movie_id + '.json', 'a+') as reviews_json:\n",
    "                    print(json.dumps(reviews), file = reviews_json, flush = True)\n",
    "\n",
    "            try:\n",
    "                # simulate click on the button to load more review \n",
    "                # and update the \"Load more\" button's key\n",
    "                driver.find_element_by_css_selector(\"button#load-more-trigger\").click()\n",
    "                wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR,\".ipl-load-more__load-indicator\")))\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                \n",
    "            except Exception:\n",
    "                break\n",
    "                \n",
    "        # close the web driver\n",
    "        driver.quit()\n",
    "        \n",
    "    print(page_count, \"pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.imdb.com/title/tt10887282/reviews - 1 pages\n",
      "https://www.imdb.com/title/tt8972556/reviews - 1 pages\n",
      "https://www.imdb.com/title/tt6853934/reviews - 1 pages\n",
      "https://www.imdb.com/title/tt8636456/reviews - 1 pages\n",
      "https://www.imdb.com/title/tt7095476/reviews - 1 pages\n",
      "https://www.imdb.com/title/tt8484012/reviews - 2 pages\n",
      "https://www.imdb.com/title/tt3246874/reviews - 4 pages\n"
     ]
    }
   ],
   "source": [
    "# for each movie with missing reviews\n",
    "for movie in movies_with_incomplete_reviews.to_dict('record'):\n",
    "    \n",
    "    # crawl reviews of current movie\n",
    "    crawl_reviews(movie['movie_id'], movie['review_url'])\n",
    "\n",
    "    # add the movie as successfully crawled\n",
    "    with open('data/review_done.csv', 'a+') as review_done:\n",
    "        print(movie['movie_id'], file = review_done, flush = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
