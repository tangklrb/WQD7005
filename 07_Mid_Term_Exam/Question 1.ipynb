{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import datetime\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to be used by web crawling processes\n",
    "\n",
    "\n",
    "def format_url(input_string):\n",
    "    if input_string is None:\n",
    "        return 'null'\n",
    "    return input_string.replace(' ', '%20').strip()\n",
    "\n",
    "\n",
    "def format_slug(input_string):\n",
    "    if input_string is None:\n",
    "        return 'null'\n",
    "    return re.sub(r'\\W+', '-', input_string.strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl data from edgeprop by url\n",
    "\n",
    "def crawl_url(url):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 '\n",
    "                      'Safari/537.36 '\n",
    "    }\n",
    "\n",
    "    request = requests.get(url, headers=headers, timeout=60)\n",
    "    request.raise_for_status()\n",
    "    response = request.json()\n",
    "\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craw POIs from GraphQL API. Calling such API is more complicated as we need to include Payload and Query\n",
    "def crawl_poi(latitude, longitude, category=None):\n",
    "    api = 'https://raptor.rea-asia.com/v1/graphql'\n",
    "\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'accept-encoding': 'gzip, deflate, br',\n",
    "        'accept-language': 'en-GB',\n",
    "        'content-type': 'application/json',\n",
    "        'market': 'MY',\n",
    "        'origin': 'https://www.iproperty.com.my',\n",
    "        'sec-fetch-mode': 'cors',\n",
    "        'sec-fetch-site': 'cross-site',\n",
    "        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36',\n",
    "        'x-market': 'ipropertymy'\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        'operationName': None,\n",
    "        'variables': {\n",
    "            'lang': 'enGB',\n",
    "            'location': str(latitude) + ',' + str(longitude),\n",
    "            'radius': 3000,\n",
    "            'pageSize': 100,\n",
    "            'category': category\n",
    "        },\n",
    "        'query': 'query ($lang: AcceptLanguage, $location: String!, $radius: Int, $pageSize: Int, $category: PoiCategory) {\\n  pois(location: $location, radius: $radius, pageSize: $pageSize, category: $category, lang: $lang) {\\n    items {\\n      name\\n      subTypeLabel\\n      subTypeExtra\\n      geometry {\\n        location {\\n          lat\\n          lng\\n          __typename\\n        }\\n        __typename\\n      }\\n      subType\\n      category\\n      lineName\\n      placeId\\n      distance\\n      distanceFloat\\n      completionYear\\n      type\\n      city\\n      district\\n      publicType\\n      curriculumOffered\\n      __typename\\n    }\\n    __typename\\n  }\\n}\\n'\n",
    "    }\n",
    "\n",
    "    request = requests.post(api, headers=headers, data=json.dumps(payload), timeout=30)\n",
    "    request.raise_for_status()\n",
    "    response = request.json()\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data directory, csv files for crawled data & csv header\n",
    "\n",
    "data_directory = 'data/'\n",
    "Path(data_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "township_csv = open(data_directory + 'q1_townships.csv', 'w+')\n",
    "transaction_csv = open(data_directory + 'q1_transactions.csv', 'w+')\n",
    "poi_csv = open(data_directory + 'q1_pois.csv', 'w+')\n",
    "\n",
    "print('project_id,asset_id,latitude,longitude,project_name,state,area,street_name,transaction_count,median_psf,median_price', flush=True, file = township_csv)\n",
    "print('project_id,project_name,transacted_price,unit_price_psf,date,property_type,tenure,floor,area_sqft,non_landed,bedrooms,street_name,psf,price,state,planning_region', flush=True, file = transaction_csv)\n",
    "print('name,sub_type_label,sub_type_extra,sub_type,category,line_name,place_id,completion_year,type,city,district,public_type,curriculum_offered,latitude,longitude', flush=True, file = poi_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing API url template for crawling data from edgeprop\n",
    "\n",
    "edgepro_api = 'https://www.edgeprop.my/jwdalice/api/v1/transactions/'\n",
    "townships_url_template = edgepro_api + 'search?&category=RESIDENTIAL&state={}&datefrom={}&dateto={}&page={}&respp=10'\n",
    "transactions_url_template = edgepro_api + 'details?&category=RESIDENTIAL&state={}&project={}&datefrom={}&dateto={}&page=1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to crawl all townships and transactions\n",
    "def crawl_townships_transactions(state, date_from, date_to):\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        # Crawl townships\n",
    "        townships_url = townships_url_template.format(format_url(state), date_from, date_to, str(page))\n",
    "        township_list = crawl_url(townships_url)\n",
    "\n",
    "        if township_list is None:\n",
    "            break\n",
    "\n",
    "        total_pages = township_list['totalpages']\n",
    "        total_townships = township_list['total']        \n",
    "        townships = township_list['property']\n",
    "\n",
    "        if page > int(total_pages):\n",
    "            break\n",
    "\n",
    "        for township in townships:\n",
    "            transaction_count = township['fieldtransactions']\n",
    "            \n",
    "            # Select fields to save into csv\n",
    "            keys_to_extract = (\n",
    "                'projectid', 'asset_id', 'lat', 'lon', 'project_name', 'state', 'area', 'street_name', \n",
    "                'fieldtransactions', 'psf', 'price'\n",
    "            )\n",
    "            # Slice to a new dict with only the selected fields\n",
    "            d = {k: str(township[k]).replace(',', '') for k in keys_to_extract}\n",
    "            \n",
    "            # Write crawled data to csv\n",
    "            print(','.join(map(str, d.values())), flush=True, file = township_csv)\n",
    "            \n",
    "            # Crawl POIs for current township\n",
    "            latitude = township['lat']\n",
    "            longitude = township['lon']\n",
    "            poi_categories = ['education', 'healthcare', 'transportation']\n",
    "\n",
    "            poi_list = list()\n",
    "            for category in poi_categories:\n",
    "                response = crawl_poi(latitude, longitude, category)\n",
    "                \n",
    "                if(\n",
    "                    (not response['data'] is None) and \n",
    "                    (not response['data']['pois'] is None) and \n",
    "                    (not response['data']['pois']['items'] is None)\n",
    "                ):\n",
    "                    for poi in response['data']['pois']['items']:\n",
    "                        try:\n",
    "                            poi['latitude'] = poi['geometry']['location']['lat']\n",
    "                            poi['longitude'] = poi['geometry']['location']['lng']\n",
    "                        except:\n",
    "                            poi['latitude'] = ''\n",
    "                            poi['longitude'] = ''\n",
    "\n",
    "                        poi_list.append(poi.copy())\n",
    "                \n",
    "            for poi in poi_list:\n",
    "                # Select fields to save into csv\n",
    "                keys_to_extract = (\n",
    "                    'name', 'subTypeLabel', 'subTypeExtra', 'subType', 'category', 'lineName', \n",
    "                    'placeId', 'completionYear', 'type', 'city', 'district', 'publicType', \n",
    "                    'curriculumOffered', 'latitude', 'longitude'\n",
    "                )\n",
    "                # Slice to a new dict with only the selected fields\n",
    "                d = {k: str(poi[k]).replace(',', '') for k in keys_to_extract}\n",
    "\n",
    "                # Write crawled data to csv\n",
    "                print(','.join(map(str, d.values())), flush=True, file = poi_csv)\n",
    "                \n",
    "            # Crawl Transactions for current township\n",
    "            project = township['project_name']\n",
    "            transaction_date_from = date_from\n",
    "            transaction_date_to = date_to\n",
    "            \n",
    "            if int(transaction_count) > 0:\n",
    "\n",
    "                while True:\n",
    "                    transactions_url = transactions_url_template.format(\n",
    "                        format_url(state), format_url(project), transaction_date_from, transaction_date_to\n",
    "                    )\n",
    "                    \n",
    "                    print('.', end = '')\n",
    "\n",
    "                    transaction_list = crawl_url(transactions_url)\n",
    "\n",
    "                    if transaction_list is None:\n",
    "                        continue\n",
    "\n",
    "                    transactions = transaction_list['property']\n",
    "                    for transaction in transactions:\n",
    "                        \n",
    "                        # Select fields to crawl\n",
    "                        keys_to_extract = (\n",
    "                            'projectid', 'project_name', 'transacted_price', 'unit_price_psf', 'date', \n",
    "                            'proptype', 'tenure', 'floor', 'area_sqft', 'non_landed', 'bedrooms', \n",
    "                            'street_name', 'psf', 'price', 'state', 'planning_region'\n",
    "                        )\n",
    "                        # Slice to a new dict with only the selected fields\n",
    "                        d = {k: str(transaction[k]).replace(',', '') for k in keys_to_extract}\n",
    "\n",
    "                        # Write crawled data to csv\n",
    "                        print(','.join(map(str, d.values())), flush=True, file = transaction_csv)\n",
    "                        \n",
    "                    if len(transaction_list['property']) == 0 or transaction_list['totalpages'] == 1:\n",
    "                        break\n",
    "\n",
    "                    date_from_unixtimestamp = datetime.datetime.strptime(transaction_date_from, '%Y-%m-%d')\n",
    "                    previous_date_to_unixtimestamp = datetime.datetime.strptime(transaction_date_to, '%Y-%m-%d')\n",
    "                    new_date_to_unixtimestamp = datetime.datetime.utcfromtimestamp(transaction_list['property'][-1]['date'])\n",
    "\n",
    "                    if previous_date_to_unixtimestamp == new_date_to_unixtimestamp:\n",
    "                        if date_from_unixtimestamp == new_date_to_unixtimestamp:\n",
    "                            break\n",
    "                        else:\n",
    "                            new_date_to_unixtimestamp = new_date_to_unixtimestamp - datetime.timedelta(days=1)\n",
    "\n",
    "                    transaction_date_to = new_date_to_unixtimestamp.strftime('%Y-%m-%d')\n",
    "\n",
    "        page = page + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program Starts: 2020-05-22 22:59:18.269819\n",
      "Crawling \"townships\", \"pois\", \"transactions\" from 2019-12-01 to 2019-12-31 for:\n",
      "KUALA LUMPUR ..............................................................................\n",
      "SELANGOR .....................................................\n",
      "PUTRAJAYA .\n",
      "Program Ends: 2020-05-22 23:05:44.392090\n"
     ]
    }
   ],
   "source": [
    "# Main programe starts\n",
    "\n",
    "print('Program Starts:', datetime.datetime.now())\n",
    "\n",
    "states = ['KUALA LUMPUR', 'SELANGOR', 'PUTRAJAYA']\n",
    "date_from = '2019-12-01'\n",
    "date_to = '2019-12-31'\n",
    "print('Crawling \"townships\", \"pois\", \"transactions\" from', date_from, 'to', date_to, 'for:')\n",
    "for state in states:\n",
    "    print(state, end = ' ')\n",
    "    crawl_townships_transactions(state, date_from, date_to)        \n",
    "    print()\n",
    "\n",
    "print('Program Ends:', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
